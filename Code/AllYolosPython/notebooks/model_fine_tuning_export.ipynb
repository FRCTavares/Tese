{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18fda0d4",
   "metadata": {},
   "source": [
    "# Model Performance Comparison & Analysis\n",
    "\n",
    "This section demonstrates how to compare different YOLO models and formats for optimal performance on Raspberry Pi 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a0eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import psutil\n",
    "import json\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üîç Model Performance Comparison Tool\")\n",
    "print(\"====================================\")\n",
    "\n",
    "# Find all available models\n",
    "model_dir = Path(\"../models\")\n",
    "model_files = []\n",
    "\n",
    "# Get PyTorch models\n",
    "pt_models = list(model_dir.glob(\"*.pt\"))\n",
    "model_files.extend(pt_models)\n",
    "\n",
    "# Get ONNX models  \n",
    "onnx_models = list(model_dir.glob(\"*.onnx\"))\n",
    "model_files.extend(onnx_models)\n",
    "\n",
    "print(f\"üì¶ Found {len(model_files)} models:\")\n",
    "for model in sorted(model_files):\n",
    "    size_mb = model.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  ‚Ä¢ {model.name}: {size_mb:.1f} MB\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready for model comparison!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39722e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_model_comparison(models_subset=None, test_duration=10, image_size=(640, 480)):\n",
    "    \"\"\"\n",
    "    Perform a quick performance comparison of YOLO models.\n",
    "    \n",
    "    Args:\n",
    "        models_subset: List of model names to test (None for all)\n",
    "        test_duration: Test duration in seconds per model\n",
    "        image_size: Input image size (width, height)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with comparison results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create test image\n",
    "    test_image = np.random.randint(0, 255, (*image_size, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Select models to test\n",
    "    if models_subset:\n",
    "        test_models = [m for m in model_files if m.name in models_subset]\n",
    "    else:\n",
    "        # Test a representative subset for quick comparison\n",
    "        test_models = [m for m in model_files if any(x in m.name for x in ['yolov8n', 'yolov5n', 'yolov10n'])]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(f\"üî• Quick comparison of {len(test_models)} models...\")\n",
    "    print(f\"‚è±Ô∏è {test_duration}s per model, {image_size} image size\\n\")\n",
    "    \n",
    "    for i, model_path in enumerate(test_models, 1):\n",
    "        model_name = model_path.name\n",
    "        print(f\"[{i}/{len(test_models)}] Testing {model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Load model\n",
    "            start_load = time.time()\n",
    "            model = YOLO(str(model_path), verbose=False)\n",
    "            load_time = time.time() - start_load\n",
    "            \n",
    "            # Warmup\n",
    "            for _ in range(3):\n",
    "                _ = model(test_image, verbose=False)\n",
    "            \n",
    "            # Benchmark\n",
    "            start_time = time.time()\n",
    "            frame_count = 0\n",
    "            inference_times = []\n",
    "            \n",
    "            while time.time() - start_time < test_duration:\n",
    "                inf_start = time.time()\n",
    "                _ = model(test_image, verbose=False)\n",
    "                inference_times.append(time.time() - inf_start)\n",
    "                frame_count += 1\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            avg_fps = frame_count / total_time\n",
    "            avg_inference_ms = (sum(inference_times) / len(inference_times)) * 1000\n",
    "            \n",
    "            # Model info\n",
    "            model_size_mb = model_path.stat().st_size / (1024 * 1024)\n",
    "            model_format = 'PyTorch' if model_path.suffix == '.pt' else 'ONNX'\n",
    "            \n",
    "            # Categorize model\n",
    "            if 'n' in model_name.lower():\n",
    "                size_category = 'Nano'\n",
    "            elif 's' in model_name.lower():\n",
    "                size_category = 'Small'\n",
    "            elif 'm' in model_name.lower():\n",
    "                size_category = 'Medium'\n",
    "            else:\n",
    "                size_category = 'Other'\n",
    "            \n",
    "            result = {\n",
    "                'Model': model_name,\n",
    "                'Format': model_format,\n",
    "                'Category': size_category,\n",
    "                'Size_MB': round(model_size_mb, 1),\n",
    "                'Load_Time_s': round(load_time, 2),\n",
    "                'FPS': round(avg_fps, 1),\n",
    "                'Inference_ms': round(avg_inference_ms, 1),\n",
    "                'FPS_per_MB': round(avg_fps / model_size_mb, 2),\n",
    "                'Frames_Tested': frame_count\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            print(f\"  ‚úÖ {avg_fps:.1f} FPS, {avg_inference_ms:.1f}ms inference\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Failed: {str(e)}\")\n",
    "            \n",
    "    return pd.DataFrame(results) if results else pd.DataFrame()\n",
    "\n",
    "# Run quick comparison\n",
    "comparison_df = quick_model_comparison(test_duration=10)\n",
    "\n",
    "if not comparison_df.empty:\n",
    "    print(\"\\nüìä Quick Comparison Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    display(comparison_df.sort_values('FPS', ascending=False))\n",
    "else:\n",
    "    print(\"‚ùå No models could be tested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8bd2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison results\n",
    "if not comparison_df.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('YOLO Model Performance Comparison on Raspberry Pi 5', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. FPS vs Model Size\n",
    "    ax1 = axes[0, 0]\n",
    "    scatter = ax1.scatter(comparison_df['Size_MB'], comparison_df['FPS'], \n",
    "                         c=comparison_df['Format'].map({'PyTorch': 'red', 'ONNX': 'blue'}),\n",
    "                         s=100, alpha=0.7)\n",
    "    ax1.set_xlabel('Model Size (MB)')\n",
    "    ax1.set_ylabel('FPS')\n",
    "    ax1.set_title('FPS vs Model Size')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add model names as annotations\n",
    "    for i, row in comparison_df.iterrows():\n",
    "        ax1.annotate(row['Model'].replace('.pt', '').replace('.onnx', ''), \n",
    "                    (row['Size_MB'], row['FPS']), \n",
    "                    xytext=(5, 5), textcoords='offset points', \n",
    "                    fontsize=8, alpha=0.8)\n",
    "    \n",
    "    # Legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='PyTorch'),\n",
    "                      Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='ONNX')]\n",
    "    ax1.legend(handles=legend_elements)\n",
    "    \n",
    "    # 2. FPS by Format\n",
    "    ax2 = axes[0, 1]\n",
    "    format_fps = comparison_df.groupby('Format')['FPS'].mean()\n",
    "    bars = ax2.bar(format_fps.index, format_fps.values, color=['red', 'blue'], alpha=0.7)\n",
    "    ax2.set_ylabel('Average FPS')\n",
    "    ax2.set_title('Average FPS by Format')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax2.annotate(f'{height:.1f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Efficiency (FPS per MB)\n",
    "    ax3 = axes[1, 0]\n",
    "    efficiency_sorted = comparison_df.sort_values('FPS_per_MB', ascending=True)\n",
    "    bars = ax3.barh(range(len(efficiency_sorted)), efficiency_sorted['FPS_per_MB'], \n",
    "                   color=efficiency_sorted['Format'].map({'PyTorch': 'red', 'ONNX': 'blue'}),\n",
    "                   alpha=0.7)\n",
    "    ax3.set_yticks(range(len(efficiency_sorted)))\n",
    "    ax3.set_yticklabels([name.replace('.pt', '').replace('.onnx', '') for name in efficiency_sorted['Model']])\n",
    "    ax3.set_xlabel('FPS per MB')\n",
    "    ax3.set_title('Model Efficiency (FPS/MB)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Inference Time vs FPS\n",
    "    ax4 = axes[1, 1]\n",
    "    scatter = ax4.scatter(comparison_df['Inference_ms'], comparison_df['FPS'],\n",
    "                         c=comparison_df['Format'].map({'PyTorch': 'red', 'ONNX': 'blue'}),\n",
    "                         s=100, alpha=0.7)\n",
    "    ax4.set_xlabel('Inference Time (ms)')\n",
    "    ax4.set_ylabel('FPS')\n",
    "    ax4.set_title('Inference Time vs FPS')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add ideal performance line\n",
    "    ideal_fps = 1000 / comparison_df['Inference_ms']  # Theoretical max FPS\n",
    "    ax4.plot(comparison_df['Inference_ms'], ideal_fps, 'g--', alpha=0.5, label='Theoretical Max')\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print analysis\n",
    "    print(\"\\nüéØ Performance Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    best_fps = comparison_df.loc[comparison_df['FPS'].idxmax()]\n",
    "    most_efficient = comparison_df.loc[comparison_df['FPS_per_MB'].idxmax()]\n",
    "    smallest = comparison_df.loc[comparison_df['Size_MB'].idxmin()]\n",
    "    \n",
    "    print(f\"üöÄ Fastest Model: {best_fps['Model']} ({best_fps['FPS']} FPS)\")\n",
    "    print(f\"‚öñÔ∏è Most Efficient: {most_efficient['Model']} ({most_efficient['FPS_per_MB']} FPS/MB)\")\n",
    "    print(f\"üì¶ Smallest Model: {smallest['Model']} ({smallest['Size_MB']} MB)\")\n",
    "    \n",
    "    # Format comparison\n",
    "    if len(comparison_df['Format'].unique()) > 1:\n",
    "        pytorch_avg = comparison_df[comparison_df['Format'] == 'PyTorch']['FPS'].mean()\n",
    "        onnx_avg = comparison_df[comparison_df['Format'] == 'ONNX']['FPS'].mean()\n",
    "        \n",
    "        if pd.notna(pytorch_avg) and pd.notna(onnx_avg):\n",
    "            improvement = ((onnx_avg - pytorch_avg) / pytorch_avg) * 100\n",
    "            print(f\"üìà ONNX vs PyTorch: {improvement:+.1f}% performance difference\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nüí° Recommendations:\")\n",
    "    print(\"=\" * 40)\n",
    "    real_time_models = comparison_df[comparison_df['FPS'] >= 10]\n",
    "    if not real_time_models.empty:\n",
    "        print(f\"üî• Real-time capable (‚â•10 FPS): {len(real_time_models)} models\")\n",
    "        for _, model in real_time_models.iterrows():\n",
    "            print(f\"   ‚Ä¢ {model['Model']}: {model['FPS']} FPS\")\n",
    "    \n",
    "    interactive_models = comparison_df[(comparison_df['FPS'] >= 5) & (comparison_df['FPS'] < 10)]\n",
    "    if not interactive_models.empty:\n",
    "        print(f\"‚ö° Interactive capable (5-10 FPS): {len(interactive_models)} models\")\n",
    "        \n",
    "    print(f\"\\nüéÆ Use Cases:\")\n",
    "    print(\"   ‚Ä¢ Real-time detection: Choose models with ‚â•10 FPS\")\n",
    "    print(\"   ‚Ä¢ Interactive applications: 5-10 FPS models acceptable\")\n",
    "    print(\"   ‚Ä¢ Batch processing: Any model suitable\")\n",
    "    print(\"   ‚Ä¢ Consider ONNX format for better performance\")\n",
    "\n",
    "else:\n",
    "    print(\"No comparison data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3898d5",
   "metadata": {},
   "source": [
    "# YOLOv8 Model Fine-Tuning and Export for Raspberry Pi 5\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Fine-tune YOLOv8-nano on a custom 5-class dataset\n",
    "2. Export the trained model to ONNX format\n",
    "3. Convert to NCNN int8 for optimized Pi 5 inference\n",
    "4. Validate performance improvements\n",
    "\n",
    "**Target Hardware:** Raspberry Pi 5 (8GB RAM) with Intel RealSense D435\n",
    "\n",
    "**Prerequisites:**\n",
    "- Custom dataset with 5 classes in YOLO format\n",
    "- Sufficient training time (recommend running on GPU first, then deploying to Pi 5)\n",
    "- Internet connection for downloading base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8eccff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# Check PyTorch and CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Set device for training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set project paths\n",
    "project_root = Path.cwd().parent\n",
    "dataset_dir = project_root / \"datasets\" / \"custom_5class\"\n",
    "models_dir = project_root / \"models\"\n",
    "export_dir = project_root / \"models\" / \"exported\"\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [dataset_dir, models_dir, export_dir]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Dataset directory: {dataset_dir}\")\n",
    "print(f\"Models directory: {models_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022e7879",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation\n",
    "\n",
    "We'll create a sample 5-class dataset for demonstration. In practice, you would replace this with your own annotated data.\n",
    "\n",
    "**Classes for this example:**\n",
    "1. `person` - Human figures\n",
    "2. `vehicle` - Cars, trucks, motorcycles  \n",
    "3. `animal` - Dogs, cats, birds\n",
    "4. `device` - Laptops, phones, cameras\n",
    "5. `furniture` - Chairs, tables, couches\n",
    "\n",
    "**Dataset Structure:**\n",
    "```\n",
    "datasets/custom_5class/\n",
    "‚îú‚îÄ‚îÄ images/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ train/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ val/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ test/\n",
    "‚îú‚îÄ‚îÄ labels/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ train/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ val/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ test/\n",
    "‚îî‚îÄ‚îÄ data.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d13531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset configuration file\n",
    "dataset_config = {\n",
    "    'train': str(dataset_dir / 'images' / 'train'),\n",
    "    'val': str(dataset_dir / 'images' / 'val'),\n",
    "    'test': str(dataset_dir / 'images' / 'test'),\n",
    "    'nc': 5,  # Number of classes\n",
    "    'names': {\n",
    "        0: 'person',\n",
    "        1: 'vehicle', \n",
    "        2: 'animal',\n",
    "        3: 'device',\n",
    "        4: 'furniture'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save dataset configuration\n",
    "config_path = dataset_dir / 'data.yaml'\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(dataset_config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"Dataset configuration saved to: {config_path}\")\n",
    "\n",
    "# Create directory structure\n",
    "for split in ['train', 'val', 'test']:\n",
    "    (dataset_dir / 'images' / split).mkdir(parents=True, exist_ok=True)\n",
    "    (dataset_dir / 'labels' / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Dataset directory structure created\")\n",
    "\n",
    "# Function to create synthetic training data (for demonstration)\n",
    "def create_synthetic_data(num_samples=50, split='train'):\n",
    "    \"\"\"Create synthetic data with random colored rectangles as objects.\"\"\"\n",
    "    images_dir = dataset_dir / 'images' / split\n",
    "    labels_dir = dataset_dir / 'labels' / split\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Create synthetic image (640x640)\n",
    "        img = np.random.randint(50, 200, (640, 640, 3), dtype=np.uint8)\n",
    "        \n",
    "        labels = []\n",
    "        num_objects = np.random.randint(1, 4)  # 1-3 objects per image\n",
    "        \n",
    "        for j in range(num_objects):\n",
    "            # Random class\n",
    "            class_id = np.random.randint(0, 5)\n",
    "            \n",
    "            # Random bounding box (YOLO format: x_center, y_center, width, height)\n",
    "            x_center = np.random.uniform(0.2, 0.8)\n",
    "            y_center = np.random.uniform(0.2, 0.8)\n",
    "            width = np.random.uniform(0.1, 0.3)\n",
    "            height = np.random.uniform(0.1, 0.3)\n",
    "            \n",
    "            # Draw colored rectangle on image (for visualization)\n",
    "            x1 = int((x_center - width/2) * 640)\n",
    "            y1 = int((y_center - height/2) * 640)\n",
    "            x2 = int((x_center + width/2) * 640)\n",
    "            y2 = int((y_center + height/2) * 640)\n",
    "            \n",
    "            color = [\n",
    "                (255, 0, 0),    # person - red\n",
    "                (0, 255, 0),    # vehicle - green  \n",
    "                (0, 0, 255),    # animal - blue\n",
    "                (255, 255, 0),  # device - yellow\n",
    "                (255, 0, 255)   # furniture - magenta\n",
    "            ][class_id]\n",
    "            \n",
    "            cv2.rectangle(img, (x1, y1), (x2, y2), color, 3)\n",
    "            \n",
    "            labels.append(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "        \n",
    "        # Save image\n",
    "        img_path = images_dir / f\"{split}_{i:04d}.jpg\"\n",
    "        cv2.imwrite(str(img_path), img)\n",
    "        \n",
    "        # Save labels\n",
    "        label_path = labels_dir / f\"{split}_{i:04d}.txt\"\n",
    "        with open(label_path, 'w') as f:\n",
    "            f.write('\\n'.join(labels))\n",
    "    \n",
    "    print(f\"Created {num_samples} synthetic {split} samples\")\n",
    "\n",
    "# Create synthetic datasets\n",
    "create_synthetic_data(100, 'train')  # 100 training images\n",
    "create_synthetic_data(20, 'val')     # 20 validation images  \n",
    "create_synthetic_data(10, 'test')    # 10 test images\n",
    "\n",
    "print(\"Synthetic dataset creation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa16ce7",
   "metadata": {},
   "source": [
    "## 2. Model Fine-Tuning\n",
    "\n",
    "Now we'll fine-tune YOLOv8-nano on our custom 5-class dataset. The training process will:\n",
    "\n",
    "1. Load the pre-trained YOLOv8n model\n",
    "2. Modify the output layer for 5 classes\n",
    "3. Train on our custom dataset\n",
    "4. Validate and save the best model\n",
    "\n",
    "**Training Parameters:**\n",
    "- Base model: YOLOv8n (nano)\n",
    "- Epochs: 50 (adjust based on convergence)\n",
    "- Image size: 640x640 (can reduce to 480x480 for Pi 5)\n",
    "- Batch size: 16 (adjust based on available memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36db470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base YOLOv8n model\n",
    "print(\"Loading YOLOv8-nano base model...\")\n",
    "model = YOLO('yolov8n.pt')  # Automatically downloads if not present\n",
    "\n",
    "# Training configuration\n",
    "train_config = {\n",
    "    'data': str(config_path),           # Path to dataset config\n",
    "    'epochs': 50,                       # Number of training epochs\n",
    "    'imgsz': 640,                       # Image size (can reduce to 480 for Pi 5)\n",
    "    'batch': 16,                        # Batch size (adjust for available RAM)\n",
    "    'workers': 4,                       # Number of data loading workers\n",
    "    'device': device,                   # Training device\n",
    "    'project': str(models_dir),         # Project directory\n",
    "    'name': 'custom_5class_v1',         # Experiment name\n",
    "    'save_period': 10,                  # Save checkpoint every N epochs\n",
    "    'patience': 10,                     # Early stopping patience\n",
    "    'optimizer': 'AdamW',               # Optimizer\n",
    "    'lr0': 0.01,                        # Initial learning rate\n",
    "    'lrf': 0.01,                        # Final learning rate fraction\n",
    "    'momentum': 0.937,                  # SGD momentum\n",
    "    'weight_decay': 0.0005,             # Weight decay\n",
    "    'warmup_epochs': 3,                 # Warmup epochs\n",
    "    'warmup_momentum': 0.8,             # Warmup momentum\n",
    "    'box': 7.5,                         # Box loss gain\n",
    "    'cls': 0.5,                         # Classification loss gain\n",
    "    'dfl': 1.5,                         # DFL loss gain\n",
    "    'pose': 12.0,                       # Pose loss gain (if applicable)\n",
    "    'kobj': 1.0,                        # Keypoint object loss gain\n",
    "    'label_smoothing': 0.0,             # Label smoothing\n",
    "    'nbs': 64,                          # Nominal batch size\n",
    "    'hsv_h': 0.015,                     # Image HSV-Hue augmentation\n",
    "    'hsv_s': 0.7,                       # Image HSV-Saturation augmentation\n",
    "    'hsv_v': 0.4,                       # Image HSV-Value augmentation\n",
    "    'degrees': 0.0,                     # Image rotation (+/- deg)\n",
    "    'translate': 0.1,                   # Image translation (+/- fraction)\n",
    "    'scale': 0.5,                       # Image scale (+/- gain)\n",
    "    'shear': 0.0,                       # Image shear (+/- deg)\n",
    "    'perspective': 0.0,                 # Image perspective (+/- fraction)\n",
    "    'flipud': 0.0,                      # Image flip up-down (probability)\n",
    "    'fliplr': 0.5,                      # Image flip left-right (probability)\n",
    "    'mosaic': 1.0,                      # Image mosaic (probability)\n",
    "    'mixup': 0.0,                       # Image mixup (probability)\n",
    "    'copy_paste': 0.0,                  # Segment copy-paste (probability)\n",
    "    'auto_augment': 'randaugment',      # Auto augmentation policy\n",
    "    'erasing': 0.4,                     # Random erasing probability\n",
    "    'crop_fraction': 1.0,               # Image crop fraction\n",
    "}\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "for key, value in train_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Start training\n",
    "print(f\"\\nStarting training on {device}...\")\n",
    "results = model.train(**train_config)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best model saved to: {results.save_dir}\")\n",
    "\n",
    "# Display training results\n",
    "best_model_path = results.save_dir / 'weights' / 'best.pt'\n",
    "print(f\"Best model path: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5b5052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best trained model for validation\n",
    "best_model = YOLO(str(best_model_path))\n",
    "\n",
    "# Validate the model\n",
    "print(\"Validating trained model...\")\n",
    "validation_results = best_model.val(data=str(config_path))\n",
    "\n",
    "print(f\"Validation mAP50: {validation_results.results_dict['metrics/mAP50(B)']:.4f}\")\n",
    "print(f\"Validation mAP50-95: {validation_results.results_dict['metrics/mAP50-95(B)']:.4f}\")\n",
    "\n",
    "# Test on a few sample images\n",
    "test_images_dir = dataset_dir / 'images' / 'test'\n",
    "test_images = list(test_images_dir.glob('*.jpg'))[:3]  # Test on first 3 images\n",
    "\n",
    "print(f\"\\nTesting on {len(test_images)} sample images...\")\n",
    "\n",
    "fig, axes = plt.subplots(1, len(test_images), figsize=(15, 5))\n",
    "if len(test_images) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, img_path in enumerate(test_images):\n",
    "    # Run inference\n",
    "    results = best_model(str(img_path))\n",
    "    \n",
    "    # Get annotated image\n",
    "    annotated = results[0].plot()\n",
    "    \n",
    "    # Convert BGR to RGB for matplotlib\n",
    "    annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    axes[i].imshow(annotated_rgb)\n",
    "    axes[i].set_title(f\"Test Image {i+1}\")\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "    # Print detection results\n",
    "    if results[0].boxes is not None:\n",
    "        boxes = results[0].boxes\n",
    "        print(f\"Image {i+1}: {len(boxes)} detections\")\n",
    "        for j, box in enumerate(boxes):\n",
    "            class_id = int(box.cls.cpu().numpy())\n",
    "            confidence = float(box.conf.cpu().numpy())\n",
    "            class_name = dataset_config['names'][class_id]\n",
    "            print(f\"  {j+1}: {class_name} ({confidence:.3f})\")\n",
    "    else:\n",
    "        print(f\"Image {i+1}: No detections\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance analysis\n",
    "model_info = best_model.info()\n",
    "print(f\"\\nModel Information:\")\n",
    "print(f\"Parameters: {model_info[0]:,}\")\n",
    "print(f\"Gradients: {model_info[1]:,}\") \n",
    "print(f\"Layers: {model_info[2]}\")\n",
    "print(f\"Model size: {best_model_path.stat().st_size / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf2c067",
   "metadata": {},
   "source": [
    "## 3. Model Export to ONNX\n",
    "\n",
    "ONNX (Open Neural Network Exchange) format provides better inference performance on CPU-based systems like the Raspberry Pi 5. We'll export our trained model to ONNX and validate the conversion.\n",
    "\n",
    "**Benefits of ONNX for Pi 5:**\n",
    "- Optimized CPU inference\n",
    "- Reduced memory usage\n",
    "- Better integration with onnxruntime\n",
    "- Cross-platform compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a2b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model to ONNX format\n",
    "print(\"Exporting model to ONNX format...\")\n",
    "\n",
    "try:\n",
    "    # Export with optimizations for Pi 5\n",
    "    onnx_path = best_model.export(\n",
    "        format='onnx',\n",
    "        imgsz=480,              # Reduced size for Pi 5 performance\n",
    "        optimize=True,          # Enable ONNX optimizations\n",
    "        simplify=True,          # Simplify ONNX graph\n",
    "        dynamic=False,          # Fixed input size for better performance\n",
    "        opset=12,              # ONNX opset version (compatible with Pi 5)\n",
    "        half=False,            # Use FP32 (Pi 5 doesn't have good FP16 support)\n",
    "        int8=False,            # We'll handle quantization separately\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(f\"ONNX model exported to: {onnx_path}\")\n",
    "    \n",
    "    # Move to export directory\n",
    "    onnx_export_path = export_dir / 'custom_5class_480.onnx'\n",
    "    shutil.copy2(onnx_path, onnx_export_path)\n",
    "    print(f\"ONNX model copied to: {onnx_export_path}\")\n",
    "    \n",
    "    # Check file size\n",
    "    onnx_size = onnx_export_path.stat().st_size / (1024*1024)\n",
    "    print(f\"ONNX model size: {onnx_size:.2f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ONNX export failed: {e}\")\n",
    "    onnx_export_path = None\n",
    "\n",
    "# Validate ONNX model if export succeeded\n",
    "if onnx_export_path and onnx_export_path.exists():\n",
    "    print(\"\\nValidating ONNX model...\")\n",
    "    \n",
    "    try:\n",
    "        import onnxruntime as ort\n",
    "        \n",
    "        # Create ONNX Runtime session\n",
    "        ort_session = ort.InferenceSession(str(onnx_export_path))\n",
    "        \n",
    "        # Get input/output info\n",
    "        input_info = ort_session.get_inputs()[0]\n",
    "        output_info = ort_session.get_outputs()\n",
    "        \n",
    "        print(f\"ONNX Input: {input_info.name} {input_info.shape} {input_info.type}\")\n",
    "        print(f\"ONNX Outputs: {len(output_info)} outputs\")\n",
    "        for i, output in enumerate(output_info):\n",
    "            print(f\"  Output {i}: {output.name} {output.shape} {output.type}\")\n",
    "        \n",
    "        # Test inference with dummy data\n",
    "        dummy_input = np.random.randn(1, 3, 480, 480).astype(np.float32)\n",
    "        outputs = ort_session.run(None, {input_info.name: dummy_input})\n",
    "        \n",
    "        print(f\"ONNX inference test successful!\")\n",
    "        print(f\"Output shapes: {[output.shape for output in outputs]}\")\n",
    "        \n",
    "        # Benchmark ONNX inference speed\n",
    "        import time\n",
    "        \n",
    "        num_runs = 10\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for _ in range(num_runs):\n",
    "            outputs = ort_session.run(None, {input_info.name: dummy_input})\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        avg_time = total_time / num_runs\n",
    "        fps = 1.0 / avg_time\n",
    "        \n",
    "        print(f\"\\nONNX Performance Benchmark:\")\n",
    "        print(f\"Average inference time: {avg_time:.4f}s\")\n",
    "        print(f\"Estimated FPS: {fps:.2f}\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"onnxruntime not available for validation\")\n",
    "    except Exception as e:\n",
    "        print(f\"ONNX validation failed: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"ONNX export failed - skipping validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cd82c2",
   "metadata": {},
   "source": [
    "## 4. Model Export to NCNN int8 (Advanced Optimization)\n",
    "\n",
    "NCNN is Tencent's optimized neural network inference framework, particularly well-suited for mobile and embedded devices like the Raspberry Pi 5. Int8 quantization further reduces model size and increases inference speed.\n",
    "\n",
    "**Benefits of NCNN int8 for Pi 5:**\n",
    "- Significantly faster inference (2-4x speedup)\n",
    "- Reduced memory usage (4x smaller)\n",
    "- Optimized for ARM processors\n",
    "- Lower power consumption\n",
    "\n",
    "**Note:** NCNN export requires additional setup and may need calibration data for int8 quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c36de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt NCNN export (requires additional dependencies)\n",
    "print(\"Attempting NCNN export...\")\n",
    "\n",
    "try:\n",
    "    # First, try direct NCNN export (if supported)\n",
    "    ncnn_path = best_model.export(\n",
    "        format='ncnn',\n",
    "        imgsz=480,\n",
    "        half=False,\n",
    "        int8=True,  # Enable int8 quantization\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    print(f\"NCNN model exported to: {ncnn_path}\")\n",
    "    \n",
    "    # NCNN export creates .param and .bin files\n",
    "    ncnn_param_path = export_dir / 'custom_5class_480.param'\n",
    "    ncnn_bin_path = export_dir / 'custom_5class_480.bin'\n",
    "    \n",
    "    # Move files to export directory\n",
    "    if isinstance(ncnn_path, str):\n",
    "        ncnn_dir = Path(ncnn_path).parent\n",
    "        param_file = list(ncnn_dir.glob('*.param'))[0]\n",
    "        bin_file = list(ncnn_dir.glob('*.bin'))[0]\n",
    "        \n",
    "        shutil.copy2(param_file, ncnn_param_path)\n",
    "        shutil.copy2(bin_file, ncnn_bin_path)\n",
    "        \n",
    "        print(f\"NCNN files copied to:\")\n",
    "        print(f\"  Param: {ncnn_param_path}\")\n",
    "        print(f\"  Binary: {ncnn_bin_path}\")\n",
    "        \n",
    "        # Check file sizes\n",
    "        param_size = ncnn_param_path.stat().st_size / 1024\n",
    "        bin_size = ncnn_bin_path.stat().st_size / (1024*1024)\n",
    "        \n",
    "        print(f\"NCNN param size: {param_size:.2f} KB\")\n",
    "        print(f\"NCNN binary size: {bin_size:.2f} MB\")\n",
    "        \n",
    "        ncnn_success = True\n",
    "    else:\n",
    "        ncnn_success = False\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Direct NCNN export failed: {e}\")\n",
    "    ncnn_success = False\n",
    "\n",
    "# Alternative: Convert ONNX to NCNN (if direct export failed)\n",
    "if not ncnn_success and onnx_export_path and onnx_export_path.exists():\n",
    "    print(\"\\nAttempting ONNX to NCNN conversion...\")\n",
    "    \n",
    "    try:\n",
    "        # This requires onnx-simplifier and custom conversion tools\n",
    "        # For now, we'll provide instructions for manual conversion\n",
    "        print(\"Manual NCNN conversion steps:\")\n",
    "        print(\"1. Install NCNN tools:\")\n",
    "        print(\"   git clone https://github.com/Tencent/ncnn.git\")\n",
    "        print(\"   cd ncnn && mkdir build && cd build\")\n",
    "        print(\"   cmake .. && make -j4\")\n",
    "        print(\"\")\n",
    "        print(\"2. Convert ONNX to NCNN:\")\n",
    "        print(f\"   ./tools/onnx/onnx2ncnn {onnx_export_path} custom_5class_480.param custom_5class_480.bin\")\n",
    "        print(\"\")\n",
    "        print(\"3. Quantize to int8 (optional):\")\n",
    "        print(\"   ./tools/quantize/ncnn2int8 custom_5class_480.param custom_5class_480.bin custom_5class_480_int8.param custom_5class_480_int8.bin calibration_images/\")\n",
    "        print(\"\")\n",
    "        print(\"Note: int8 quantization requires calibration images from your dataset\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ONNX to NCNN conversion guidance failed: {e}\")\n",
    "\n",
    "# Create calibration dataset for int8 quantization\n",
    "print(\"\\nPreparing calibration data for int8 quantization...\")\n",
    "\n",
    "calibration_dir = export_dir / 'calibration_images'\n",
    "calibration_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Copy a subset of validation images for calibration\n",
    "val_images_dir = dataset_dir / 'images' / 'val'\n",
    "val_images = list(val_images_dir.glob('*.jpg'))[:10]  # Use 10 images for calibration\n",
    "\n",
    "for i, img_path in enumerate(val_images):\n",
    "    # Load and resize image to model input size\n",
    "    img = cv2.imread(str(img_path))\n",
    "    img_resized = cv2.resize(img, (480, 480))\n",
    "    \n",
    "    # Save calibration image\n",
    "    calib_path = calibration_dir / f\"calib_{i:03d}.jpg\"\n",
    "    cv2.imwrite(str(calib_path), img_resized)\n",
    "\n",
    "print(f\"Created {len(val_images)} calibration images in {calibration_dir}\")\n",
    "\n",
    "# Provide summary of export options\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL EXPORT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Original PyTorch model: {best_model_path}\")\n",
    "print(f\"Model size: {best_model_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "if onnx_export_path and onnx_export_path.exists():\n",
    "    print(f\"\\nONNX model: {onnx_export_path}\")\n",
    "    print(f\"ONNX size: {onnx_export_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "    print(\"‚úì Ready for onnxruntime inference\")\n",
    "\n",
    "if ncnn_success:\n",
    "    print(f\"\\nNCNN model: {ncnn_param_path}, {ncnn_bin_path}\")\n",
    "    print(\"‚úì Ready for NCNN inference\")\n",
    "else:\n",
    "    print(f\"\\nNCNN conversion: Manual steps required\")\n",
    "    print(\"üìù See conversion instructions above\")\n",
    "\n",
    "print(f\"\\nCalibration data: {calibration_dir}\")\n",
    "print(f\"Calibration images: {len(list(calibration_dir.glob('*.jpg')))}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8f331a",
   "metadata": {},
   "source": [
    "## 5. Performance Validation and Deployment\n",
    "\n",
    "Let's validate the performance of our exported models and provide guidance for deploying them to the Raspberry Pi 5 pipeline.\n",
    "\n",
    "### Model Comparison\n",
    "We'll compare the performance characteristics of different model formats:\n",
    "- **PyTorch (.pt)**: Original format, full precision\n",
    "- **ONNX (.onnx)**: Optimized for CPU inference\n",
    "- **NCNN (.param/.bin)**: Mobile/embedded optimized with int8 quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eca63e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison function\n",
    "def benchmark_model(model_path, model_type, num_runs=20):\n",
    "    \"\"\"Benchmark model inference performance.\"\"\"\n",
    "    import time\n",
    "    \n",
    "    # Prepare test image\n",
    "    test_img = np.random.randn(1, 3, 480, 480).astype(np.float32)\n",
    "    \n",
    "    if model_type == 'pytorch':\n",
    "        model = YOLO(str(model_path))\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(3):\n",
    "            model(test_img)\n",
    "        \n",
    "        # Benchmark\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_runs):\n",
    "            results = model(test_img)\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "    elif model_type == 'onnx':\n",
    "        try:\n",
    "            import onnxruntime as ort\n",
    "            session = ort.InferenceSession(str(model_path))\n",
    "            input_name = session.get_inputs()[0].name\n",
    "            \n",
    "            # Warmup\n",
    "            for _ in range(3):\n",
    "                session.run(None, {input_name: test_img})\n",
    "            \n",
    "            # Benchmark\n",
    "            start_time = time.time()\n",
    "            for _ in range(num_runs):\n",
    "                outputs = session.run(None, {input_name: test_img})\n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"onnxruntime not available\")\n",
    "            return None\n",
    "            \n",
    "    else:\n",
    "        print(f\"Model type {model_type} not supported for benchmarking\")\n",
    "        return None\n",
    "    \n",
    "    avg_time = total_time / num_runs\n",
    "    fps = 1.0 / avg_time\n",
    "    \n",
    "    return {\n",
    "        'avg_inference_time': avg_time,\n",
    "        'fps': fps,\n",
    "        'total_time': total_time,\n",
    "        'num_runs': num_runs\n",
    "    }\n",
    "\n",
    "# Benchmark available models\n",
    "print(\"Benchmarking model performance...\")\n",
    "print(\"Note: This simulates inference on current hardware, not Pi 5\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models_to_test = [\n",
    "    (best_model_path, 'pytorch', 'Custom PyTorch'),\n",
    "]\n",
    "\n",
    "if onnx_export_path and onnx_export_path.exists():\n",
    "    models_to_test.append((onnx_export_path, 'onnx', 'Custom ONNX'))\n",
    "\n",
    "benchmark_results = {}\n",
    "\n",
    "for model_path, model_type, display_name in models_to_test:\n",
    "    print(f\"\\nBenchmarking {display_name}...\")\n",
    "    result = benchmark_model(model_path, model_type)\n",
    "    \n",
    "    if result:\n",
    "        benchmark_results[display_name] = result\n",
    "        print(f\"  Average inference time: {result['avg_inference_time']:.4f}s\")\n",
    "        print(f\"  Estimated FPS: {result['fps']:.2f}\")\n",
    "        print(f\"  File size: {model_path.stat().st_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Create performance comparison chart\n",
    "if benchmark_results:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    models = list(benchmark_results.keys())\n",
    "    fps_values = [benchmark_results[model]['fps'] for model in models]\n",
    "    inference_times = [benchmark_results[model]['avg_inference_time'] * 1000 for model in models]  # Convert to ms\n",
    "    \n",
    "    # FPS comparison\n",
    "    ax1.bar(models, fps_values, color=['blue', 'orange'][:len(models)])\n",
    "    ax1.set_ylabel('FPS')\n",
    "    ax1.set_title('Model FPS Comparison')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Inference time comparison\n",
    "    ax2.bar(models, inference_times, color=['blue', 'orange'][:len(models)])\n",
    "    ax2.set_ylabel('Inference Time (ms)')\n",
    "    ax2.set_title('Model Inference Time Comparison')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Expected Pi 5 performance estimates\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RASPBERRY PI 5 PERFORMANCE ESTIMATES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "pi5_estimates = {\n",
    "    'Custom PyTorch': {'fps_range': '8-12', 'memory': '2-3 GB'},\n",
    "    'Custom ONNX': {'fps_range': '12-18', 'memory': '1.5-2.5 GB'},\n",
    "    'Custom NCNN int8': {'fps_range': '15-25', 'memory': '1-2 GB'}\n",
    "}\n",
    "\n",
    "for model_name, estimates in pi5_estimates.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Expected FPS: {estimates['fps_range']}\")\n",
    "    print(f\"  Memory usage: {estimates['memory']}\")\n",
    "\n",
    "print(f\"\\nFactors affecting Pi 5 performance:\")\n",
    "print(f\"- CPU temperature (thermal throttling above 80¬∞C)\")\n",
    "print(f\"- Power supply quality (5V/5A recommended)\")\n",
    "print(f\"- USB bandwidth for camera (USB 3.0 required)\")\n",
    "print(f\"- Background processes and system load\")\n",
    "print(f\"- Model complexity and input resolution\")\n",
    "\n",
    "# Generate deployment instructions\n",
    "deployment_instructions = f\"\"\"\n",
    "# DEPLOYMENT TO RASPBERRY PI 5\n",
    "\n",
    "## 1. Copy Models to Pi 5\n",
    "```bash\n",
    "# Copy trained model files to Pi 5\n",
    "scp {best_model_path} pi@your-pi5:/home/pi/real-time-object-detection/models/\n",
    "\"\"\"\n",
    "\n",
    "if onnx_export_path and onnx_export_path.exists():\n",
    "    deployment_instructions += f\"scp {onnx_export_path} pi@your-pi5:/home/pi/real-time-object-detection/models/\\n\"\n",
    "\n",
    "deployment_instructions += \"\"\"\n",
    "```\n",
    "\n",
    "## 2. Update Detection Pipeline\n",
    "Edit `src/detector.py` to use your custom model:\n",
    "\n",
    "```python\n",
    "# In Detector.__init__()\n",
    "self.model_path = \"models/custom_5class_480.onnx\"  # Use ONNX for best performance\n",
    "self.class_names = ['person', 'vehicle', 'animal', 'device', 'furniture']\n",
    "```\n",
    "\n",
    "## 3. Run Custom Detection\n",
    "```bash\n",
    "# Activate environment\n",
    "source venv/bin/activate\n",
    "\n",
    "# Run with custom model\n",
    "python3 src/main.py --model models/custom_5class_480.onnx --confidence 0.6\n",
    "\n",
    "# Or with PyTorch model\n",
    "python3 src/main.py --model models/best.pt --confidence 0.6\n",
    "```\n",
    "\n",
    "## 4. Performance Optimization\n",
    "- Use ONNX model for best CPU performance\n",
    "- Reduce input resolution if needed: `--width 320 --height 240`\n",
    "- Increase confidence threshold: `--confidence 0.7`\n",
    "- Enable headless mode for maximum performance: `--headless`\n",
    "\"\"\"\n",
    "\n",
    "print(deployment_instructions)\n",
    "\n",
    "# Save deployment instructions to file\n",
    "deployment_file = export_dir / 'deployment_instructions.md'\n",
    "with open(deployment_file, 'w') as f:\n",
    "    f.write(deployment_instructions)\n",
    "\n",
    "print(f\"\\nDeployment instructions saved to: {deployment_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bc36a6",
   "metadata": {},
   "source": [
    "## 6. Summary and Next Steps\n",
    "\n",
    "### What We Accomplished\n",
    "\n",
    "1. **Created a Custom Dataset**: Generated a synthetic 5-class dataset for demonstration\n",
    "2. **Fine-tuned YOLOv8-nano**: Trained on custom classes with optimized parameters\n",
    "3. **Exported to Multiple Formats**: \n",
    "   - PyTorch (.pt) for development and testing\n",
    "   - ONNX (.onnx) for optimized CPU inference on Pi 5\n",
    "   - NCNN (.param/.bin) for maximum mobile optimization\n",
    "4. **Performance Validation**: Benchmarked models and estimated Pi 5 performance\n",
    "5. **Deployment Ready**: Provided complete deployment instructions\n",
    "\n",
    "### Model Performance Summary\n",
    "\n",
    "| Format | Size | Expected Pi 5 FPS | Memory Usage | Best For |\n",
    "|--------|------|-------------------|--------------|----------|\n",
    "| PyTorch | ~12 MB | 8-12 FPS | 2-3 GB | Development |\n",
    "| ONNX | ~6 MB | 12-18 FPS | 1.5-2.5 GB | Production |\n",
    "| NCNN int8 | ~3 MB | 15-25 FPS | 1-2 GB | Maximum Performance |\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Deploy to Pi 5**: Copy models and update the detection pipeline\n",
    "2. **Real-world Testing**: Test with actual camera data and adjust thresholds\n",
    "3. **Dataset Improvement**: Replace synthetic data with real annotated images\n",
    "4. **Hyperparameter Tuning**: Optimize training parameters for your specific use case\n",
    "5. **Advanced Optimizations**: \n",
    "   - Model pruning to remove unnecessary parameters\n",
    "   - Knowledge distillation for smaller models\n",
    "   - TensorRT optimization (if using Jetson instead of Pi 5)\n",
    "\n",
    "### Troubleshooting Tips\n",
    "\n",
    "- **Low Training Accuracy**: Increase dataset size, check label quality, adjust learning rate\n",
    "- **Export Failures**: Ensure compatible PyTorch/ONNX versions, check CUDA availability\n",
    "- **Pi 5 Performance Issues**: Reduce input resolution, increase confidence threshold, check thermal throttling\n",
    "- **Memory Errors**: Use smaller batch sizes, enable swap, reduce model complexity\n",
    "\n",
    "### Resources for Further Learning\n",
    "\n",
    "- [Ultralytics YOLOv8 Documentation](https://docs.ultralytics.com/)\n",
    "- [ONNX Model Optimization](https://onnxruntime.ai/docs/performance/model-optimizations/)\n",
    "- [NCNN Framework Guide](https://github.com/Tencent/ncnn/wiki)\n",
    "- [Raspberry Pi 5 Performance Optimization](https://www.raspberrypi.org/documentation/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
